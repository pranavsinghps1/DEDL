{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms as tsfm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import seed_everything\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\"\"\"\n",
    "Some parts of the code are based on https://www.kaggle.com/code/qkn123/efficientb4-inference\n",
    "Implementtion of CoAtNet:https://arxiv.org/abs/2106.04803\n",
    "and Pytorch implementation from https://www.kaggle.com/code/thedevastator/train-infer-coatnet-efficientnet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # data path\n",
    "    train_csv_path = '/scratch/ps4364/Colton/New data/Metadata/train_list.csv'\n",
    "    #train_list.csv only has train split\n",
    "    train_imgs_dir = '/scratch/ps4364/Colton/New data/backup/Classification_images/'\n",
    "    # label info\n",
    "    label_num2str = {0: 'b',\n",
    "                     1: 'tfh',\n",
    "                     2: 'tfh217',\n",
    "                     3: 'tfhl',\n",
    "                     4: 'other'}\n",
    "    label_str2num = {'b': 0,\n",
    "                     'tfh': 1,\n",
    "                     'tfh217': 2,\n",
    "                     'tfhl': 3,\n",
    "                     'other': 4}\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight = [0.9475164011246484, 0.4934395501405811, 0.5029053420805999, 0.2, 1.0]\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='swin_large_patch4_window12_384'\n",
    "    seed = 77\n",
    "    num_classes = 5\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    accum_grad_batch = 1\n",
    "    early_stop_delta = 1e-7\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the code cell below to normalize weights for focal loss based on class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr, t_min, t_max):\n",
    "    norm_arr = []\n",
    "    diff = t_max - t_min\n",
    "    diff_arr = max(arr) - min(arr)\n",
    "    for i in arr:\n",
    "        temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "        norm_arr.append(temp)\n",
    "    return norm_arr\n",
    "  \n",
    "# assign array and range\n",
    "array_1d = [0.944, 0.4595, 0.4696, 0.1464, 1]\n",
    "range_to_normalize = (0.2, 1)\n",
    "normalized_array_1d = normalize(\n",
    "    array_1d, range_to_normalize[0], \n",
    "  range_to_normalize[1])\n",
    "  \n",
    "# display original and normalized array\n",
    "print(\"Original Array = \", array_1d)\n",
    "print(\"Normalized Array = \", normalized_array_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define train & valid image transformation\n",
    "\"\"\"\n",
    "DATASET_IMAGE_MEAN = (0.485, 0.456, 0.406)\n",
    "DATASET_IMAGE_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomAffine(degrees=10),], p=0.3),\n",
    "                                tsfm.RandomVerticalFlip(p=0.3),\n",
    "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n",
    "valid_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define dataset class\n",
    "\"\"\"\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, cfg, img_names: list, labels: list, transform=None):\n",
    "        self.img_dir = cfg.train_imgs_dir\n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx]+'.jpg')\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_ts = self.transform(img)\n",
    "        label_ts = self.labels[idx]\n",
    "        return img_ts, label_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Focal-Loss\n",
    "\"\"\"\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        #f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        rec = self.tp/(self.tp + self.fn)\n",
    "        return rec\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = pd.read_csv(CFG.train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split train & validation into Cross-Validation Folds\n",
    "\"\"\"\n",
    "\n",
    "all_img_names: list = TRAIN_DF[\"file\"].values.tolist()\n",
    "all_img_labels: list = TRAIN_DF[\"label\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_labels_ts = []\n",
    "for tmp_lb in all_img_labels:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    j = tmp_lb[1:-1]\n",
    "    for a in j:\n",
    "        if a != ',' and  a !=' ':\n",
    "            k=int(a)\n",
    "            tmp_label[k] = 1.0\n",
    "    all_img_labels_ts.append(tmp_label)\n",
    "    \n",
    "k_fold = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF = pd.read_csv('/scratch/ps4364/Colton/New data/Metadata/test_list.csv')\n",
    "all_imgtest_names: list = TEST_DF[\"file\"].values.tolist()\n",
    "all_imgtest_labels: list = TEST_DF[\"label\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=CFG()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CoAtNet architecture Definition\"\"\"\n",
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup))\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),                \n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup))\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample: return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else: return x + self.conv(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "        self.ih, self.iw = image_size\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5       \n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw))\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample: x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else: x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = [2, 2, 12, 28, 2]\n",
    "channels = [64, 64, 128, 256, 512]\n",
    "model = CoAtNet((384, 384), 3, num_blocks, channels, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
    "    print('Initialising Model..........')\n",
    "    num_blocks = [2, 2, 12, 28, 2]\n",
    "    channels = [64, 64, 128, 256, 512]\n",
    "    model = CoAtNet((384, 384), 3, num_blocks, channels, num_classes=5)\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight = [0.9475164011246484, 0.4934395501405811, 0.5029053420805999, 0.2, 1.0]\n",
    "    criterion_vit = FocalLoss(fl_alpha, fl_gamma)\n",
    "    model.to(device)\n",
    "    print('*'*10)\n",
    "    print('For',fold_idx)\n",
    "    print('Initialising Dataset..........')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr)\n",
    "    fold_train_img_names = [all_img_names[idx] for idx in train_indices]\n",
    "    fold_valid_img_names = [all_img_names[idx] for idx in valid_indices]\n",
    "    fold_train_img_labels = [all_img_labels_ts[idx] for idx in train_indices]\n",
    "    fold_valid_img_labels = [all_img_labels_ts[idx] for idx in valid_indices]\n",
    "    train_dataset = Dataset(CFG, fold_train_img_names, fold_train_img_labels, train_transform)\n",
    "    valid_dataset = Dataset(CFG, fold_valid_img_names, fold_valid_img_labels, valid_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "    criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "    metric = MyF1Score(cfg)\n",
    "    val_metric=MyF1Score(cfg)\n",
    "    best=0\n",
    "    best_val=0\n",
    "    total_loss=0\n",
    "    counter=0\n",
    "    prev=math.inf\n",
    "    for epoch in tqdm(range(20)):\n",
    "        model.train()\n",
    "        for images,label in train_loader:\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            model.to(device)\n",
    "            pred_ts=model(images)\n",
    "            loss = criterion(pred_ts, label)\n",
    "            score = metric(pred_ts,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            total_loss+=loss.item()\n",
    "        avg_loss=total_loss/len(train_loader)\n",
    "        train_score=metric.compute()\n",
    "        logs = {'train_loss': avg_loss, 'train_f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "        print(logs)\n",
    "        if best < train_score:\n",
    "            best=train_score\n",
    "            model.eval()\n",
    "            total_loss=0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                pred_ts=model(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                total_loss+=loss.item()\n",
    "            avg_val_loss=total_loss/len(valid_loader)\n",
    "            print('Val Loss:',avg_val_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('ViT Validation Score:',val_score)\n",
    "            if avg_val_loss < prev:\n",
    "                counter=0\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model,\n",
    "                        './{}-{}.pt'.format('CoAtNet',fold_idx))\n",
    "            else:\n",
    "                counter+=1\n",
    "            print('Counter:',counter)\n",
    "            prev=avg_val_loss\n",
    "            if counter > 5:\n",
    "                print(\"Early-Stopping!\")\n",
    "                break\n",
    "                    \n",
    "    # Training the Corresponding ViT\n",
    "        print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps_env",
   "language": "python",
   "name": "ps_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}